{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''\nAdam Forestier\nLast Updated: May 7, 2023\n'''\n\n# Imports\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns \n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier) \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (accuracy_score, recall_score, confusion_matrix, classification_report, ConfusionMatrixDisplay,RocCurveDisplay, roc_curve, auc)\nfrom sklearn.model_selection import (train_test_split, GridSearchCV)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-07T23:35:59.522363Z","iopub.execute_input":"2023-05-07T23:35:59.522726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Initial Data Investigation and Tuning","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/diabetes-prediction-dataset/diabetes_prediction_dataset.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Info\ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Can see there are no null values. Let's ensure there are no duplicates\ndf = df.drop_duplicates()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Every age has .0 in decimal place. Let's convert to an integer as there is no month or day data; only age in years \ndf['age'] = np.vectorize(lambda age: int(age))(df['age'])\ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# general data statistics\ndf.describe().transpose()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I see that both gender and smoking_history are string objects. \n# The model will perform better with both features are binary objects\n# I keep the original data frame for more readable visualizations\nfinal_df = pd.get_dummies(df, drop_first=True) \nfinal_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Investigate the balance of the data\ndf['diabetes'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The data is highly imbalanced. We will likely need to handle this imbalance in order to have high performing models..**","metadata":{}},{"cell_type":"code","source":"# Let's investigate the correlation of each feature to the label we are trying to predict: diabetes\ndiabetes_correlation = final_df.corr()['diabetes'].sort_values()[:-1]\ndiabetes_correlation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**It appears blood_glucose_level and HbA1c_level are the two highest correlated features to diabetes**","metadata":{}},{"cell_type":"markdown","source":"### Data Exploration through Visualization","metadata":{}},{"cell_type":"code","source":"# View Relationship between two strongest correlated feature and the label\nsns.scatterplot(x='HbA1c_level', y='blood_glucose_level', data=final_df, hue='diabetes')\nplt.xlabel('HbA1c Level')\nplt.ylabel('Blood Glucose Level')\nplt.title('Diabetic vs Non-Diabetic by HbA1c and Blood Glucose')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We can see a very clear seperation between diabetics and non-diabetics based on HbA1c level and blood glucose level.**\n\n**This indicates to me, that a simple KNN model, or may be a good algorithms for this classification task. Lets Explore this relationship further.**","metadata":{}},{"cell_type":"code","source":"# Boxplot of blood glucose and diabetes\nsns.boxplot(x='diabetes', y='blood_glucose_level', data=final_df)\nplt.xlabel('Diabetic')\nplt.ylabel('Blood Glucose Level')\nplt.title('Blood Glucose for Diabetic and Non-Diabetic')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Boxplot of blood glucose and diabetes\nsns.boxplot(x='diabetes', y='HbA1c_level', data=final_df)\nplt.xlabel('Diabetic')\nplt.ylabel('HbA1c Level')\nplt.title('HbA1c for Diabetic and Non-Diabetic')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**high blood glucose and HbA1C_level are strong indicators of diabetes**","metadata":{}},{"cell_type":"markdown","source":"**This indicates to me, that a simple KNN model, or may be a good algorithms for this classification task. Lets Explore this relationship further.**","metadata":{}},{"cell_type":"code","source":"# Distribution of ages for those with and without diabetes\nsns.displot(data=final_df, x='age', bins=50, col='diabetes', hue='diabetes')\nplt.title('Distribution of Ages for Diabetics and Non-Diabetics')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution of BMI for those with and without diabetes\nsns.displot(data=final_df, x='bmi', bins=50, col='diabetes', hue='diabetes')\nplt.title('Distribution of Ages for Diabetics and Non-Diabetics')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View count of diabetics and non-diabetics by hypertension \nsns.countplot(data=final_df, x='hypertension', hue='diabetes')\nplt.xlabel('Hypertension')\nplt.ylabel('Total Count')\nplt.title('Hypertension and Diabetes')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View count of diabetics and non-diabetics by heart disease \nsns.countplot(data=final_df, x='heart_disease', hue='diabetes')\nplt.xlabel('Diabetic')\nplt.ylabel('Total Count')\nplt.title('Heart Disease and Diabetes')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count of diabetics and non diabetics by gender\nsns.countplot(data=df, x='gender', hue='diabetes')\nplt.xlabel('Diabetic')\nplt.ylabel('Total Count')\nplt.title('Gender and Diabetes')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count of diabetics and non diabetics by gender\nsns.countplot(data=df, x='smoking_history', hue='diabetes')\nplt.xlabel('Diabetic')\nplt.ylabel('Total Count')\nplt.title('Smoking History and Diabetes')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Count for those with heart disease and hypertension with no diabetes exceeds those with both and heart disease...**\n**HOWEVER - we must remember the unbalanced dataset. They are near equal AND there are only 1/10 the amount of those with diabetes in the dataset.**","metadata":{}},{"cell_type":"markdown","source":"### Classification Models","metadata":{}},{"cell_type":"markdown","source":"**With all of the following visualized, let's start training some models with the findings we have gathered**\n\n**We are going to start with high bias and low variance (low complexity) and increase complexity**\n**..The first model will be a very simple K Nearest Neighbors model utilizing only Blood Glucose and HbA1c**","metadata":{}},{"cell_type":"code","source":"# Seperate features and label. \nX = final_df[['blood_glucose_level', 'HbA1c_level']]\ny = final_df['diabetes']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33, random_state=101)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale data. Only fit training data to prevent data leakage\nscaler = StandardScaler()\nscaled_X_train = scaler.fit_transform(X_train)\nscaled_X_test = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a K Nearest Neighbors model with search for optimum amount of neighbors between 1 - 10. Use Minkowski algorithm for distance calculation\n# We are trying to accurately predict when someone has diabetes. Use recall as the scoring metric\nknn_clf = KNeighborsClassifier()\nknn_clf.fit(scaled_X_train, y_train)\ny_pred = knn_clf.predict(scaled_X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion matrix to display precision and recall. \nknn_cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\np = ConfusionMatrixDisplay(confusion_matrix=knn_cm, display_labels=knn_clf.classes_)\np.plot()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification Report\nknn_cr = classification_report(y_true=y_test, y_pred=y_pred)\nprint(knn_cr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**This classifier performs well in most facets. High precision and recall for non-diabetics. Overall accuracy of 97%**\n\n**The classifier also always 100% correct when assigning the diabetic label to a patient. However it is missing 1/3 of the positive cases!**\n\n**There are too many false negatives. Considering the task of this model, to identify when individuals have diabetes. We need to have higher recall for class 1, even if it is at the expense of other scores**\n\n**Let us see if employing SMOTE, will help us improve our Recall...**","metadata":{}},{"cell_type":"code","source":"# Balance data using SMOTE\nsm = SMOTE(random_state=2)\nX, y = sm.fit_resample(X,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33, random_state=101)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale data. Only fit training data to prevent data leakage\nscaler = StandardScaler()\nscaled_X_train = scaler.fit_transform(X_train)\nscaled_X_test = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a K Nearest Neighbors model with search for optimum amount of neighbors between 1 - 10. Use Minkowski algorithm for distance calculation\n# We are trying to accurately predict when someone has diabetes. Use recall as the scoring metric\nknn_clf = KNeighborsClassifier()\nknn_clf.fit(scaled_X_train, y_train)\ny_pred = knn_clf.predict(scaled_X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion matrix to display precision and recall. \nknn_cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\np = ConfusionMatrixDisplay(confusion_matrix=knn_cm, display_labels=knn_clf.classes_)\np.plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification Report\nprint(classification_report(y_true=y_test, y_pred=y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Hmm... Our recall has vastly improved for class 1, but at the cost of all other scores.**","metadata":{}},{"cell_type":"markdown","source":"**Let's investigate if Hyperplanes perform stronger at seperating diabetics vs. non-diabetics than Neighbors**","metadata":{}},{"cell_type":"code","source":"# Support Vector Classifier using the 5 strongest correlated features\nstrongest_correlated_features = ['heart_disease', 'hypertension', 'bmi', 'age', 'HbA1c_level', 'blood_glucose_level']\nX = final_df[strongest_correlated_features]\ny = final_df['diabetes']\nX, y = sm.fit_resample(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33, random_state=101)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale data. Only fit training data to prevent data leakage\nscaler = StandardScaler()\nscaled_X_train = scaler.fit_transform(X_train)\nscaled_X_test = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a Support Vector Machine Classifier with cross validation\nsvm_clf = SVC()\nsvm_clf.fit(scaled_X_train, y_train)\ny_pred = svm_clf.predict(scaled_X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion matrix to display precision and recall. Calculate accuracy as well\nsvm_clf_cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\np = ConfusionMatrixDisplay(confusion_matrix=svm_clf_cm, display_labels=svm_clf.classes_)\np.plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification Report\nsvm_clf_cr = classification_report(y_true=y_test, y_pred=y_pred)\nprint(svm_clf_cr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The Support Vector Machine Classifier is support the KNN classifier with higher Precision and Recall Scores for both diabetics and non-diabetics. I think that we can still do better though**","metadata":{}},{"cell_type":"markdown","source":"**Let's try some Ensemble approaches**\n\n**We are going to train RandomForest, AdaBoost, and GradientBoost classifiers. For these models, we will use every feature to try to predict our label**","metadata":{}},{"cell_type":"code","source":"# Seperate features and label\nX = final_df.drop('diabetes', axis=1)\ny = final_df['diabetes']\nX, y = sm.fit_resample(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33, random_state=101)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**NOTE: We do not need to scale for tree based models**","metadata":{}},{"cell_type":"code","source":"# random forest\nforest_clf = RandomForestClassifier(random_state=101, oob_score=True)\nforest_clf.fit(X=X_train, y=y_train)\nhighest_y_pred = forest_clf.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion matrix to display precision and recall.\nforest_clf_cm = confusion_matrix(y_true=y_test, y_pred=highest_y_pred)\np = ConfusionMatrixDisplay(confusion_matrix=forest_clf_cm, display_labels=forest_clf.classes_)\np.plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification Report\nforest_clf_cr = classification_report(y_true=y_test, y_pred=highest_y_pred)\nprint(forest_clf_cr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"forest_clf.oob_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**That's more like it! 97% across the board for the classification report; including an out of bag score of 96.8%**","metadata":{}},{"cell_type":"markdown","source":"**AdaBoost Classifier**","metadata":{}},{"cell_type":"code","source":"# Classifier with the lowest recall error\nada_clf = AdaBoostClassifier(random_state=101)\nada_clf.fit(X_train, y_train)\ny_pred = ada_clf.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show the features with importance > 0\nfeature_imp = pd.DataFrame(index=X.columns, data=ada_clf.feature_importances_, columns=['Importance'])\nfeature_imp = feature_imp[feature_imp['Importance'] > 0.0001]\nfeature_imp = feature_imp.sort_values('Importance')\nsns.barplot(x=feature_imp.index, y='Importance', data=feature_imp)\nplt.xlabel('Feature')\nplt.xlabel('Importance')\nplt.xticks(rotation=90)\nplt.title('Importance by Feature')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion matrix to display precision and recall.\nada_cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\np = ConfusionMatrixDisplay(confusion_matrix=ada_cm, display_labels=ada_clf.classes_)\np.plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification Report\nada_cr = classification_report(y_true=y_test, y_pred=y_pred)\nprint(ada_cr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Another high performing model. Ensemble models appear to be performing well** ","metadata":{}},{"cell_type":"markdown","source":"**Let's try a GradientBoost Model**","metadata":{}},{"cell_type":"code","source":"clf = GradientBoostingClassifier()\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion matrix to display precision and recall. \ngradient_cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\np = ConfusionMatrixDisplay(confusion_matrix=gradient_cm, display_labels=clf.classes_)\np.plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification Report\ngradient_cr = classification_report(y_true=y_test, y_pred=y_pred)\nprint(gradient_cr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**All three ensemble learners performed well. Gradient Boost is the second best model thus far, with excellent recall on class 0 and precision on class 1. It's overall accuracy is just below Random Forest however**","metadata":{}},{"cell_type":"markdown","source":"**Final Model: Cross Validated LogisticRegression utizing ElasticNet Regularization**","metadata":{}},{"cell_type":"code","source":"# Scale data for logistic regression\nscaler = StandardScaler()\nscaled_X_train = scaler.fit_transform(X_train)\nscaled_X_test = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression w/ ElasticNet Regularization\nmodel = LogisticRegression(penalty='elasticnet', l1_ratio=.99, C=1, solver='saga')\nmodel.fit(X=scaled_X_train, y=y_train)\ny_pred = model.predict(scaled_X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion matrix to display precision and recall. \nlog_cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\np = ConfusionMatrixDisplay(confusion_matrix=log_cm, display_labels=model.classes_)\np.plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification Report\nlog_cr = classification_report(y_true=y_test, y_pred=y_pred)\nprint(log_cr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Logistic Regression is not as accurate for classifying this data set as ensemble learners**","metadata":{}},{"cell_type":"markdown","source":"### Summary of Findings","metadata":{}},{"cell_type":"markdown","source":"Prior to using SMOTE to balance the dataset,  models performed with a high level of accuracy, but low recall. This is the result of an imbalanced dataset, i.e. many more non-diabetic individuals than diabetic. \n\nAfter balancing the dataset using SMOTE, a Random Forest Classifier performed the best with 97% accuracy, precision and recall!","metadata":{}},{"cell_type":"markdown","source":"Thank you for taking the time to review my notebook! Please feel free to leave any questions, comments, or critiques! ","metadata":{}}]}